{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOG9+d3FBm+gQuLC/fgc2nn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisaac21/TensorFlow/blob/main/Recurrent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Natural Language Processing**\n",
        "\n",
        "NLP is computing that deals with trying to understand actual human language. Some examples are translations, autofill, spellcheck, etc.\n",
        "\n",
        "##Recurrent Neural Networks\n",
        "\n",
        "In this tutorial we will introduce a new kind of neural network that is much more capable of processing sequential data such as text or characters called a **recurrent neural network** (RNN for short). \n",
        "\n",
        "They work by using *sequential memory*. When we learn the alphabet, we learn it as a sequence. So saying it in full is easy, but trying to say it backwards is very tough. Moreover, if we start at an arbitrary letter, it will be hard to get the next few letters but then the rest will follow quickly as we picked up on the pattern. "
      ],
      "metadata": {
        "id": "pqa6UdTBwACm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Ecoding Textual Data**\n",
        "\n",
        "##Bag of Words\n",
        "\n",
        "In this algorithm, we create a vocabulary (list of words), and use a number that represents every single word. In a large data set, this can mean a very large dictionary. \n",
        "  * We only track words that we have seen, and how frequently they occur. We loose the order of the words, but we have the frequency. It's like throwing all the words that appear into a bag.\n",
        "\n",
        "```I thought the movie was going to be bad, but it was actually amazing!```\n",
        "\n",
        "```I thought the movie was going to be amazing, but it was actually bad!```\n",
        "\n",
        "These two sentences have the exact same words and frequency, but have very different meanings. We loose that meaning using bag of words because these two would be encoded the same. "
      ],
      "metadata": {
        "id": "YoBHOmgl1LQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
        "  bag = {}  # stores all of the encodings and their frequency\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      encoding = vocab[word]  # get encoding from vocab\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding = word_encoding\n",
        "      word_encoding += 1\n",
        "    \n",
        "    if encoding in bag:\n",
        "      bag[encoding] += 1\n",
        "    else:\n",
        "      bag[encoding] = 1\n",
        "  \n",
        "  return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ],
      "metadata": {
        "id": "u-FsCCx_3DOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae077051-4e8a-4bed-8eb8-b8bebbb0b6ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Integer Encoding\n",
        "\n",
        "Let's say we want to keep the order. What if we just created a list of the string, with each integer representing a word. That way, we would have a list of integers and we don't loose the place of each word. "
      ],
      "metadata": {
        "id": "TMsaGL_e3UBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}  \n",
        "word_encoding = 1\n",
        "def one_hot_encoding(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \") \n",
        "  encoding = []  \n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      code = vocab[word]  \n",
        "      encoding.append(code) \n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding.append(word_encoding)\n",
        "      word_encoding += 1\n",
        "  \n",
        "  return encoding\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "encoding = one_hot_encoding(text)\n",
        "print(encoding)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfEEYzYC3z2g",
        "outputId": "b377c096-2d16-4c94-9318-0c2f2ecf6804"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, let's say we have a large vocabulary of 100,000 words. Let's say we have the word `happy` encoded as 1, and `good` encoded as 100,000. When we pass in a sentence that has happy vs good, the model is going to struggle to realize they are similar as they are so numerically far apart. \n",
        "\n",
        "The numbers we choose for each words is important. "
      ],
      "metadata": {
        "id": "0jYWXDVj36xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Words Embedding\n",
        "\n",
        "This encoder tries to find words that are similar and assign them similar values. This algorithm assigns a vector to each word, where each component will represent how similar the overall word is to each component word. \n",
        "\n",
        "This would be a layer in our model, so our model would actually learn how to embed the vocabulary. "
      ],
      "metadata": {
        "id": "2As_jT--4o3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Recurrent Neural Networks (RNN's)**\n",
        "\n",
        "The big difference between an RNN vs a Densly connected or Convolutional Neural Network is RNN's contain a loop within the layers. \n",
        "\n",
        "This gives the RNN an 'internal memory' of what is has already seen.  \n",
        "\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
        "\n",
        "What these variables stand for...\n",
        "\n",
        "h<sub>t</sub> output at time t\n",
        "\n",
        "x<sub>t</sub> input at time t\n",
        "\n",
        "A Recurrent Layer (loop)\n",
        "\n",
        "What we are doing is we process our first word to generate some output. We then use that output in conjuction with our second input/word to produce a better understanding of the two words together. \n",
        "\n",
        "What we've just looked at is called a **simple RNN layer**. It can be effective at processing shorter sequences of text for simple problems but has many downfalls associated with it. For longer strings of input, the RNN struggles to remember earlier words, and they 'fade out' of memory. There are some methods to combat this. "
      ],
      "metadata": {
        "id": "zMpmTBmS5tro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Long-Short Term Memory (LSTM)\n",
        "\n",
        "What we do here is add another layer to track the internal state. With a simple RNN, all we do is use the previous output (which is all the previous words + the latest ones mushed together) to generate a new output. With LSTM, this other layer allows us to look back at the things we saw at the beginning (or remember the earlier parts). This allows us to make more useful predictions. \n",
        "\n",
        "It chooses what words/info are most important to remember, and keeps that in the internal state. So not only do we pass in the output from previous layer, we also pass in that internal state and combine it with the previous output. \n"
      ],
      "metadata": {
        "id": "OrPLeKVY7Ypy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Building Sentiment Analysis Model**\n",
        "\n"
      ],
      "metadata": {
        "id": "V-atlt7qc1nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.datasets import imdb\n",
        "from keras.utils import pad_sequences\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584 # num of words we will include\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5ZRoj5RdFA8",
        "outputId": "a41996cd-5d4d-4318-b630-a3c340ba8290"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing\n",
        "\n",
        "Our reviews come in different lengths. To pass it to our model, there needs to be length of 250 words. To fix this we will\n",
        "\n",
        "* Trim off extra words if review is > 250 words\n",
        "* Add neccessary 0's if review is < 250 words (*padding*)\n"
      ],
      "metadata": {
        "id": "YRzhkhLLderr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pad_sequences(train_data, MAXLEN)\n",
        "test_data = pad_sequences(test_data, MAXLEN)"
      ],
      "metadata": {
        "id": "aFoGqZwGd7Ae"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[1] # now array length is 250"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF7maNnseFcP",
        "outputId": "eea4851f-130f-44ee-b1e0-5bc6e055e9f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     1,   194,\n",
              "        1153,   194,  8255,    78,   228,     5,     6,  1463,  4369,\n",
              "        5012,   134,    26,     4,   715,     8,   118,  1634,    14,\n",
              "         394,    20,    13,   119,   954,   189,   102,     5,   207,\n",
              "         110,  3103,    21,    14,    69,   188,     8,    30,    23,\n",
              "           7,     4,   249,   126,    93,     4,   114,     9,  2300,\n",
              "        1523,     5,   647,     4,   116,     9,    35,  8163,     4,\n",
              "         229,     9,   340,  1322,     4,   118,     9,     4,   130,\n",
              "        4901,    19,     4,  1002,     5,    89,    29,   952,    46,\n",
              "          37,     4,   455,     9,    45,    43,    38,  1543,  1905,\n",
              "         398,     4,  1649,    26,  6853,     5,   163,    11,  3215,\n",
              "       10156,     4,  1153,     9,   194,   775,     7,  8255, 11596,\n",
              "         349,  2637,   148,   605, 15358,  8003,    15,   123,   125,\n",
              "          68, 23141,  6853,    15,   349,   165,  4362,    98,     5,\n",
              "           4,   228,     9,    43, 36893,  1157,    15,   299,   120,\n",
              "           5,   120,   174,    11,   220,   175,   136,    50,     9,\n",
              "        4373,   228,  8255,     5, 25249,   656,   245,  2350,     5,\n",
              "           4,  9837,   131,   152,   491,    18, 46151,    32,  7464,\n",
              "        1212,    14,     9,     6,   371,    78,    22,   625,    64,\n",
              "        1382,     9,     8,   168,   145,    23,     4,  1690,    15,\n",
              "          16,     4,  1355,     5,    28,     6,    52,   154,   462,\n",
              "          33,    89,    78,   285,    16,   145,    95], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Model"
      ],
      "metadata": {
        "id": "hGER7Nt-eMW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(VOCAB_SIZE, 32), # creates vectors  \n",
        "  tf.keras.layers.LSTM(32), # 32 shows the num of dimensions each vector has\n",
        "  # and then implements the LSTM algo \n",
        "  tf.keras.layers.Dense(1, activation='sigmoid') \n",
        "  # since sigmoid moves values between 0-1, we can have a certain range\n",
        "  # represent negative, and the rest of range represent positive\n",
        "])"
      ],
      "metadata": {
        "id": "5yzr7kyLeNpG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9l3cnByfSqi",
        "outputId": "32a58f38-2f08-4c2e-b10c-a4745f5360ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and Testing"
      ],
      "metadata": {
        "id": "hdM5oeg9fZqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkdL-CDafa2T",
        "outputId": "99f5aedf-208f-407d-a9b5-a785a8c84265"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 13ms/step - loss: 0.4400 - acc: 0.8029 - val_loss: 0.3350 - val_acc: 0.8596\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.2449 - acc: 0.9052 - val_loss: 0.3373 - val_acc: 0.8744\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.1874 - acc: 0.9304 - val_loss: 0.2731 - val_acc: 0.8896\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.1529 - acc: 0.9462 - val_loss: 0.2848 - val_acc: 0.8912\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.1292 - acc: 0.9556 - val_loss: 0.2821 - val_acc: 0.8924\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.1100 - acc: 0.9614 - val_loss: 0.3400 - val_acc: 0.8944\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.0938 - acc: 0.9686 - val_loss: 0.3223 - val_acc: 0.8890\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.0841 - acc: 0.9716 - val_loss: 0.3122 - val_acc: 0.8942\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.0737 - acc: 0.9766 - val_loss: 0.4026 - val_acc: 0.8868\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.0664 - acc: 0.9783 - val_loss: 0.3377 - val_acc: 0.8858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('sentiment_analysis_FCC.h5') #h5 is unqiue to keras"
      ],
      "metadata": {
        "id": "SxAeVK1ifwa8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that after one epoch of training, the validation accuracy remains pretty consistent and doesn't improve. This means we would need to update the model and create a better one. "
      ],
      "metadata": {
        "id": "htefWNxuhksC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG9ZDIwlhmVP",
        "outputId": "e839718f-8fc1-4360-98b7-19c92a9760e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 6ms/step - loss: 0.4204 - acc: 0.8585\n",
            "[0.4204045236110687, 0.8584799766540527]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Making Predictions"
      ],
      "metadata": {
        "id": "2TwoeCDPgYLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index() # shows us index of each word\n",
        "\n",
        "# for preprocessing a line of text\n",
        "def encode(text):\n",
        "  # given some text, convert each word into a token (sepeartae each out)\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  # if the word that is in the toxen is in mapping, replace word with index, \n",
        "  # else put 0. \n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  # Then pad token sequence (put words on a list, so just take first element)\n",
        "  return pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = 'that movie was just amazing, so amazing'\n",
        "encoded = encode(text)\n",
        "print(encoded)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQf0rNAyga0l",
        "outputId": "5458e88e-bfb3-4c96-b797-e72e7c2deaac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also make a decoder..."
      ],
      "metadata": {
        "id": "gfHkho3Uhb1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_index = {value : key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode(encode):\n",
        "  PAD = 0\n",
        "  text = \"\"\n",
        "  for num in encode:\n",
        "    if num != PAD:\n",
        "      text += reverse_word_index[num] + \" \"\n",
        "  return text[: -1] # don't include last space \n",
        "\n",
        "print(decode(encoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw50b56NhdYs",
        "outputId": "d436b79b-5798-42cd-94dd-d371588684a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that movie was just amazing so amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's make a prediction"
      ],
      "metadata": {
        "id": "TQq3lwd7iYn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "  encoded_text = encode(text)\n",
        "  pred = np.zeros((1,250)) # putting in format model expects \n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred)\n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was awesome! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsWBqKOciZ7g",
        "outputId": "93b521ac-92b1-4a15-b78d-96aa23fb84cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 385ms/step\n",
            "[0.7320799]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "[0.32456654]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the positive review, let's remove the words so awesome. Let's see how that changes the text "
      ],
      "metadata": {
        "id": "k1FM_UqEjYsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhPLl1yOjcfj",
        "outputId": "e32e8f98-8977-4fde-8b2b-4c8f09449d49"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "[0.8693845]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actually makes the prediction much better! This could be an indication of where our model is starting to fail"
      ],
      "metadata": {
        "id": "B96Ip_Bcjomj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**RNN Play Generator**\n",
        "\n",
        "This model is predicting the most likely next character. We will train the model on a bunch of sequences from play Romeo and Juliet, so that it learns how to predict next character\n",
        "\n",
        "In order to generate the play, we are going to recursively feed back the model's output back as an input to the model, and it will keep predicting next character until a play has been created. "
      ],
      "metadata": {
        "id": "CVXqER8Mk9X5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset\n",
        "\n",
        "Getting the data from keras"
      ],
      "metadata": {
        "id": "_ssChn9NlzUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Could use any play\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22oJZ5Csl2_n",
        "outputId": "fbaac675-4fce-434e-be10-c60ffc726de4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's open the file and look at the contents"
      ],
      "metadata": {
        "id": "ZR8hl3mXmCKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zTE7pNqmEeZ",
        "outputId": "175567d4-3b7b-49ac-b68b-62eae3999fb5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250]) # let's look at the formatting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G88VBBFYmJ1y",
        "outputId": "84a56ac0-fb2e-47ef-c552-1b96beb043cc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing/Encoding"
      ],
      "metadata": {
        "id": "-XTfaWZemRCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are simply encoding each character, not each word\n",
        "# This makes our lives easier as there are a finite amount of characters\n",
        "\n",
        "vocab = sorted(set(text)) #sorts all unique characters\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)} # creates a mapping \n",
        "# 0 : first char\n",
        "# 1 : second char...\n",
        "idx2char = np.array(vocab) # sorts in same order\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "PS8-lRVUmTeI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how first 14 lettters gets encoded"
      ],
      "metadata": {
        "id": "4VJwjhmQnQ08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:14])\n",
        "print(\"Encoded:\", text_to_int(text[:14]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfcHdMKsnU9m",
        "outputId": "45069214-03c0-41d3-8971-b3b44fbf0648"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen:\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:14]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSw5bkIZngGJ",
        "outputId": "f42074a1-b4c4-4d06-cf86-e894d84bbb15"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Training Examples\n",
        "\n",
        "We have to feed in lines from the play, not a play as a whole. This means we needs to split our text data into shorter sequences \n",
        "\n",
        "We will prepare using a *seq_length* sequence as input and *seq_length* sequence as the output where that sequence is the original sequence shifted one letter to the right. For example:\n",
        "\n",
        "```input: Hell | output: ello```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pMyWOHIVokvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length+1)\n",
        "# each input is 100, each output is 100, so need the extra\n",
        "# 1 char\n",
        "\n",
        "# let's create the dataset - coverts the string into characters\n",
        "# creates a stream of characters\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "xDV8oae6pKMA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "# taking char data set and batching it, and drops excess char"
      ],
      "metadata": {
        "id": "kJ5N7fP_p7Jz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to use sequences and split then into input and output"
      ],
      "metadata": {
        "id": "Lh9_e1PoqFpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "hb1hjjKHqIQA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make the training batches"
      ],
      "metadata": {
        "id": "ZsTvH6FfqhNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "WPlFEQ6Xqi9D"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building the Model\n",
        "\n",
        "We are going to create a function that creates the model. We are doing this because then we can create different sized batches. We will later see how the different inputs change the model's play"
      ],
      "metadata": {
        "id": "b9IeRhH8rCHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim,\n",
        "                rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential(\n",
        "      [\n",
        "       tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                 batch_input_shape=[batch_size, None]),\n",
        "       tf.keras.layers.LSTM(rnn_units,\n",
        "                              return_sequences=True, # returns each step\n",
        "                              stateful=True,\n",
        "                              recurrent_initializer='glorot_uniform'),\n",
        "       tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwsAftk_rTTl",
        "outputId": "6591d45f-87e2-4826-b730-fa3e653d376b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So everytime we pass the model an input, we are giving it ```BATCH_SIZE``` (in this case 64) examples. Each example is *seq_length* (in this case 100) long. "
      ],
      "metadata": {
        "id": "RmZBbaBgu3ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating a Loss Function\n",
        "\n",
        "Let's first better understand what outputs our model will create\n",
        "\n"
      ],
      "metadata": {
        "id": "LhmINzisvT3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYdV6NAuv6KV",
        "outputId": "13676660-c5aa-4b99-d564-9584d8da84a9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfudV-GjxF7E",
        "outputId": "5e1a750e-228d-4e55-a94b-1cee88801edb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 3.68090812e-04 -5.45515039e-04 -4.90843784e-03 ...  7.13919336e-03\n",
            "   -2.80073262e-03  2.41264468e-03]\n",
            "  [-2.09684111e-03 -2.40268372e-03 -5.97841153e-03 ...  6.36724615e-03\n",
            "   -3.80303035e-03 -2.23946478e-03]\n",
            "  [-4.20775125e-03 -3.39156203e-03 -7.21301930e-03 ...  6.19579479e-03\n",
            "   -4.99288738e-03 -5.11343405e-03]\n",
            "  ...\n",
            "  [ 1.80198089e-03  7.82457180e-03 -1.07970126e-02 ...  5.05239842e-03\n",
            "    2.16252077e-03 -1.01939007e-03]\n",
            "  [ 9.30761918e-03  8.40969197e-03 -9.96937323e-03 ... -3.85008357e-03\n",
            "   -3.99610959e-03  4.19853721e-04]\n",
            "  [ 7.09580025e-03  8.55452567e-03 -9.40575637e-03 ...  6.89452514e-04\n",
            "    1.48233376e-03 -1.95975765e-03]]\n",
            "\n",
            " [[ 1.20801618e-03  1.00468635e-03  4.14339913e-04 ...  1.72446505e-03\n",
            "   -2.88173999e-03 -7.61428615e-03]\n",
            "  [ 1.78394967e-03  2.84323841e-03  7.80719856e-04 ...  3.17022065e-03\n",
            "   -4.80477605e-03 -1.29608754e-02]\n",
            "  [-4.23035957e-03  4.45083808e-03  1.30832214e-02 ...  5.79528464e-03\n",
            "   -8.81895330e-03 -1.32872723e-02]\n",
            "  ...\n",
            "  [ 7.97397271e-03  5.73231652e-03 -9.85952839e-03 ... -5.18458011e-03\n",
            "   -1.01036215e-02  2.05462612e-03]\n",
            "  [ 5.25028631e-03  7.56753329e-03 -5.03930543e-03 ... -5.45684062e-03\n",
            "   -1.08217858e-02  2.57392414e-03]\n",
            "  [ 5.23691624e-03  5.58833545e-03 -3.44836782e-03 ... -1.07819913e-04\n",
            "   -8.67409818e-03  5.77282673e-03]]\n",
            "\n",
            " [[ 6.77292293e-04  1.72940572e-03  4.46606241e-03 ... -8.59593041e-04\n",
            "   -1.95466657e-03  1.58487819e-03]\n",
            "  [ 2.58965394e-03  2.74629053e-03  2.86112702e-03 ...  3.10886605e-03\n",
            "    2.75338534e-04 -2.21234444e-03]\n",
            "  [-3.51647759e-04  3.89496842e-03  2.84406822e-04 ...  2.67158193e-03\n",
            "    4.04787017e-03  1.44806376e-03]\n",
            "  ...\n",
            "  [-1.08368946e-02  6.87915972e-03 -7.39164790e-03 ...  4.48059710e-03\n",
            "   -9.70943552e-03 -4.67164349e-03]\n",
            "  [-9.22453310e-03  4.52197483e-03 -1.11201871e-02 ...  7.44669093e-03\n",
            "   -1.55482590e-02 -6.33231364e-04]\n",
            "  [-5.98580809e-03  7.54392985e-03 -3.85494344e-03 ...  4.98829503e-03\n",
            "   -1.40971607e-02  1.01158977e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 3.68090812e-04 -5.45515039e-04 -4.90843784e-03 ...  7.13919336e-03\n",
            "   -2.80073262e-03  2.41264468e-03]\n",
            "  [ 1.67458737e-03  2.33930442e-03  9.72890994e-04 ...  4.52598417e-03\n",
            "   -4.84496122e-03  2.60034343e-03]\n",
            "  [ 2.80589028e-03  2.00207927e-03 -3.73241166e-03 ...  1.09825153e-02\n",
            "   -6.08796580e-03  4.20572702e-03]\n",
            "  ...\n",
            "  [ 7.21490011e-04  5.87428408e-03 -9.01619159e-03 ... -8.32832418e-03\n",
            "    2.28615291e-03  3.72635433e-03]\n",
            "  [ 8.32949299e-04  4.82801441e-03 -1.17865074e-02 ... -2.73427577e-04\n",
            "    6.26784749e-05  4.87678219e-03]\n",
            "  [ 1.70688890e-03  6.79761916e-03 -1.00641809e-02 ...  1.70106860e-03\n",
            "    4.83685452e-03  3.03522684e-04]]\n",
            "\n",
            " [[-3.08131287e-03 -3.04037845e-03 -2.51535093e-03 ...  9.72736452e-04\n",
            "   -9.00013314e-04 -3.24229454e-03]\n",
            "  [-2.73261103e-03  5.82716544e-04 -9.55236435e-04 ... -1.89412781e-03\n",
            "    1.42314692e-03  1.19942450e-03]\n",
            "  [-3.39619769e-03  2.64123222e-03 -1.55345816e-03 ...  1.71983754e-03\n",
            "    9.28302412e-04 -4.83095972e-03]\n",
            "  ...\n",
            "  [ 7.66921695e-03  7.72570074e-03 -1.01611577e-02 ... -4.87764366e-03\n",
            "   -1.00586098e-02 -1.84958149e-03]\n",
            "  [ 6.41284138e-03  8.40058271e-03 -4.10177791e-03 ... -4.68029454e-03\n",
            "   -1.00239422e-02  4.16692579e-04]\n",
            "  [ 7.54389167e-03  5.74853923e-03 -9.23397299e-03 ... -3.07387533e-03\n",
            "   -3.19305575e-03 -2.44492665e-03]]\n",
            "\n",
            " [[-2.16250261e-03  2.75470363e-03  1.28595950e-03 ...  1.72906241e-03\n",
            "    2.58470140e-03 -2.12986744e-03]\n",
            "  [ 1.03168539e-04  3.41155101e-04 -4.62539308e-03 ... -3.90073773e-03\n",
            "   -1.22428720e-03  2.05013435e-03]\n",
            "  [-3.45816440e-03  2.34182202e-03 -3.17909010e-03 ... -5.06739924e-03\n",
            "   -7.06533156e-03 -1.16147404e-03]\n",
            "  ...\n",
            "  [-4.73839091e-03  1.20706391e-02 -1.36686000e-03 ...  5.06778574e-03\n",
            "   -4.03984357e-03 -4.68874024e-03]\n",
            "  [-8.55211262e-03  1.32527035e-02 -3.84553452e-03 ...  5.62542398e-03\n",
            "   -1.51532446e-03 -2.93224701e-03]\n",
            "  [-9.84534062e-03  1.38526894e-02 -2.48730765e-03 ...  6.74554752e-03\n",
            "    6.09040027e-04 -4.96512465e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each \n",
        "# interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCfwOtKexH3b",
        "outputId": "b7ed4ee0-1a5a-4b00-e701-5611e7593763"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00036809 -0.00054552 -0.00490844 ...  0.00713919 -0.00280073\n",
            "   0.00241264]\n",
            " [-0.00209684 -0.00240268 -0.00597841 ...  0.00636725 -0.00380303\n",
            "  -0.00223946]\n",
            " [-0.00420775 -0.00339156 -0.00721302 ...  0.00619579 -0.00499289\n",
            "  -0.00511343]\n",
            " ...\n",
            " [ 0.00180198  0.00782457 -0.01079701 ...  0.0050524   0.00216252\n",
            "  -0.00101939]\n",
            " [ 0.00930762  0.00840969 -0.00996937 ... -0.00385008 -0.00399611\n",
            "   0.00041985]\n",
            " [ 0.0070958   0.00855453 -0.00940576 ...  0.00068945  0.00148233\n",
            "  -0.00195976]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity \n",
        "# of each character occuring next after first timestep "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtrOeBo9xLBs",
        "outputId": "17cacee2-918e-48ed-dcc9-34ff6d6627fa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 0.00036809 -0.00054552 -0.00490844  0.00105465  0.00344535  0.00285257\n",
            " -0.00434119 -0.0015713  -0.00268748 -0.00070672 -0.00102871 -0.00231994\n",
            " -0.00057037  0.00291047 -0.00723727 -0.0060222  -0.00126017  0.00334753\n",
            "  0.00142394 -0.0004067   0.00238988  0.00448186 -0.00355611  0.00278146\n",
            "  0.00039315 -0.00267827  0.00158394 -0.00085192 -0.00333688  0.00620662\n",
            "  0.0037038   0.00047582  0.00285353  0.00117921  0.00084248 -0.00375288\n",
            " -0.00719812 -0.00687876 -0.00300554 -0.00034162 -0.00527214 -0.00118199\n",
            " -0.00220543 -0.00337661 -0.00150805  0.0010796  -0.00444193 -0.0030654\n",
            " -0.00442922 -0.0047661   0.00053501 -0.00440049  0.00361999  0.00113765\n",
            "  0.00246774 -0.00471719 -0.00215014  0.00052043  0.00480846  0.00131714\n",
            " -0.00098852 -0.00871481  0.00713919 -0.00280073  0.00241264], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to \n",
        "# sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all\n",
        "#  the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0] \n",
        "# we are sampling the distribution, instead of just picking largest %\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  \n",
        "# and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7fCB-olAxMB2",
        "outputId": "ba14ecc3-a9ca-4914-9b19-8bd7e27d6324"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"iT?K$jI3BsSEGqIhiqpekW&yVgOjKUpV'GOsE3-HSUXrsn;kv3edxx-u,Q,uB;;RPhmzm,a$ncue!DmWNT-R-\\nxPAqq'oudLhu$-\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is why we have to build our own loss function,  because our model is creating a very very sepcific output "
      ],
      "metadata": {
        "id": "jKMT9F6Vxtc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "EXXDza-4ym9j"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compiling the Model"
      ],
      "metadata": {
        "id": "v88ssKSayxL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "SLMPS2iVyzlN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create checkpoints for the model as it trains. This allow us to load the model from a checkpoint and continue training it"
      ],
      "metadata": {
        "id": "0KLvhfIwy9eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "7nTIaLSlzCA9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Model"
      ],
      "metadata": {
        "id": "sJc9alSqzE4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R29dZxH7zGDi",
        "outputId": "7dd07657-c612-4f96-8ecf-15c3ffa1e611"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 14s 65ms/step - loss: 2.6126\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 12s 65ms/step - loss: 1.8987\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 13s 66ms/step - loss: 1.6483\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.5114\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.4298\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 1.3734\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 1.3279\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.2887\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2520\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2177\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.1841\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.1478\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.1100\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.0715\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.0320\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.9902\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.9500\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.9085\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.8682\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.8285\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.7921\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.7573\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.7263\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6947\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6666\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6429\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6201\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5998\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5798\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5639\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5487\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5362\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5243\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5117\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5033\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4933\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4860\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4773\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4724\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4632\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4574\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4544\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4510\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4445\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.4441\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4390\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4336\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4312\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4289\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the Model\n",
        "\n",
        "After we have trained the model, we want to rebuild the modelfrom the latest checkpoint (therby loading the weights). "
      ],
      "metadata": {
        "id": "Kq5obX9ZzXvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "fD9x7h25ztwq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "EQ4JgGbxzyTM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load **any checkpoint** we want by specifying the exact file to load."
      ],
      "metadata": {
        "id": "bNZOBR840BKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "xHRtyoGc0Aqk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating Text"
      ],
      "metadata": {
        "id": "ZRMVb43N0GnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0) #makes a nested list\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states() # reseting the states, so it doesn't remember last \n",
        "  # state after generating data \n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0) #Removes exterior dimension\n",
        "\n",
        "      # using a categorical distribution to predict \n",
        "      # the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated)) # return the generated play"
      ],
      "metadata": {
        "id": "ycI3cEFa0IHK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "id": "oRLosaBy0ycY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31950329-65ed-4f5e-c749-14c7329a8a27"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: start\n",
            "start part.\n",
            "\n",
            "LUCENTIO:\n",
            "And then?\n",
            "\n",
            "BIONDELLO:\n",
            "His but a murder, master, in you moved.\n",
            "\n",
            "LEONTES:\n",
            "O Paulina,\n",
            "Come, God shall go. Bear thes on thee;\n",
            "Jespire the prince: and hope the rest were r ing me to this amazent,\n",
            "To pardon him, but that e'er I know thou wilt.\n",
            "\n",
            "LADY GREY:\n",
            "I take my leave the winger of the sword,\n",
            "When he doth find what he did spend and at the last,\n",
            "I set up my trust.\n",
            "\n",
            "First Citizen:\n",
            "So stood the state more prot protectors of thine eyes;\n",
            "Perform'd poor citizens and the royal father Verona's sake.\n",
            "\n",
            "HASTINGS:\n",
            "No; I beseech you,\n",
            "Commend me to my father bring him at your voices,\n",
            "I'll point you from the Minand speak for Lancaster.\n",
            "And Gaunt, his blood. My joyiers,\n",
            "Find him, and he shall go.\n",
            "\n",
            "GLOUCESTER:\n",
            "Look, here, sir; here comes Keeper:\n",
            "Say What says your opinion!\n",
            "\n",
            "ISABELLA:\n",
            "O, beli\n"
          ]
        }
      ]
    }
  ]
}